{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Customer Feedback Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "The airline industry is highly competitive and customer service is a critical area that can influence customer loyalty and brand reputation. Timely and effective handling of customer feedback is crucial for maintaining service quality. Analyzing customer feedback can provide insights into common concerns and areas for improvement.\n",
    "\n",
    "### Project Goals\n",
    "\n",
    "The primary goal of this project is twofold:\n",
    "\n",
    "1. **Sentiment Analysis**: To automatically determine the sentiment of customer comments. The sentiment can be:\n",
    "   - **Negative (Complaint)**: Indicates the customer faced an issue or is dissatisfied.\n",
    "   - **Positive (Congratulation)**: Reflects customer satisfaction or a positive experience.\n",
    "   - **Neutral (Opinion)**: Represents a customer's opinion that is neither a direct complaint nor praise.\n",
    "\n",
    "2. **Complaint Classification**: To classify customer feedback into categories for routing to the appropriate department. This classification will help in streamlining the process of addressing customer feedback efficiently. Example categories include:\n",
    "   - **Luggage Issues**: For complaints or feedback related to damaged or lost luggage.\n",
    "   - **Flight Delays**: For issues pertaining to delayed or cancelled flights.\n",
    "   - **Billing Issues**: For concerns related to overcharges, refunds, or payment discrepancies.\n",
    "\n",
    "The end product will be a model or a set of models that can accurately tag incoming customer comments with the correct sentiment and category, allowing the customer service team to promptly address and resolve the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os.path\n",
    "import json\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import little_mallet_wrapper as lmw\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "path_to_mallet = 'mallet-2.0.8/bin/mallet'  # CHANGE THIS TO YOUR MALLET PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gabot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_excel('output.xlsx', engine='openpyxl')\n",
    "\n",
    "# Change typo in column name\n",
    "data.rename(columns={'descripciom': 'descripcion'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords for both English and Spanish\n",
    "english_stopwords = stopwords.words('english')\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "all_stopwords = set(english_stopwords + spanish_stopwords)\n",
    "# Add additional Spanish stopwords\n",
    "unwanted_words = []\n",
    "with open('list_stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    new_stopwords = json.load(file)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to string in case there are non-string types\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML-like tags: <anything>\n",
    "    for word in unwanted_words:\n",
    "        text = re.sub(r'\\b' + re.escape(word) + r'\\b', '', text) # Remove specific unwanted words\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-záéíóúñ\\s]', '', text)  # Remove punctuation and special chars\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "\n",
    "    # Remove stopwords for both English and Spanish\n",
    "    text = ' '.join(word for word in text.split() if word not in all_stopwords)\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'descripcion' column\n",
    "data['cleaned_descripcion'] = data['descripcion'].astype(str).apply(clean_text)\n",
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gracias escribirnos recibido correo seguida daremos respuesta emergencias '\n",
      " 'favor comunicate']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = data.cleaned_descripcion.values.tolist()\n",
    "\n",
    "# Remove non-string elements\n",
    "data = [str(sent) for sent in data]\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gracias', 'escribirnos', 'recibido', 'correo', 'seguida', 'daremos', 'respuesta', 'emergencias', 'favor', 'comunicate']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gracias', 'escribirnos_recibido_correo', 'seguida_daremos_respuesta', 'emergencias', 'favor', 'comunicate']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estarías', 'hasn', 'tenías', 'habremos', 'noches', 'esos', 'hubiera', \"you've\", 'una', 'ella', 'isn', 'up', 'sois', 'tenidas', 'png', 'ain', \"needn't\", 'solo', 'ourselves', 'suyos', 'now', 'tenidos', 'cid', \"shan't\", 'sentid', 'sus', 'uno', 'los', \"you'll\", \"isn't\", 'tenía', 'hayáis', 'estéis', 'correo electrónico', 'a', 'sin', 'esta', 'te', \"doesn't\", 'hubieses', 'muchas', 'formato', \"wasn't\", 'era', 'habrían', 'adjunto podrá', 'sea', 'seríais', 'favorito', 'sí', 'hubiéramos', 'where', 'vuestra', 'estábamos', 'ustede', 'most', \"hasn't\", 'but', 'gmail', 'esa', 'son', 'ls', 'qué', 'from', 'mis', 'de', 'contra', 'do', 'div', 'donde', 'estados', 'habré', 'otros', 'www', 'quedo', 'how', 'estarán', 'm', 'important', 'before', 'eran', 'feature', 'tiene', 'días', 'the', \"weren't\", 'porque', 'seré', 'tenga', 'cual', 'estado', 'suya', 'estuve', 'hn_evehu', 'siente', 'habríamos', 'mail_itinerary', 'having', 'which', 'by', 'tened', 'noche', 'tendréis', 'les', 'those', 'será', 'en', \"mightn't\", 'content', 'habréis', 'cognitoforms', 'tuvisteis', 'todo', 'more', 'esté', 'them', 'estás', 'none', 'habrías', 'mías', 'serán', 'in', 'estuviéramos', 'tr', 'yours', 'won', 'soy', 'sentidas', 'hacer', 'os', 'what', 'at', 'estamos', 'muy', 'tuviésemos', 'tuvieran', 'yo', 'net', 'own', 'same', 'needn', 're', 'fuimos', 'estada', 'o', 'image004', 'y', 'favor', 'otras', 'be', 'mía', 'me', 'términos', 'itself', 'algunas', 'only', 'lo', 'rt', 'hubo', 'estén', 'estuviste', 'mío', 'estaremos', 'tengáis', 'its', 'table', 'auto', 'hubiésemos', 'during', 'tienen', 'tendrían', 'size', 'fuese', 'ni', 'does', 'shouldn', 'nbsp', 'algo', 'nuestro', 'tuvieses', 'podrá', 'todos', 'habido', 'vea', 'tendríais', 'https', 'hotmail', \"didn't\", 'estabas', 'seremos', 'ours', 'tuya', 'herself', 'tendrá', 'our', 'been', 'if', 'habidos', 'hasta', 'shan', 'as', 'tenéis', 'just', 'estaban', 'tanto', 'esas', 'through', 'estuvieras', 'theirs', 'haven', 'e', 'nuestros', 'again', 'fls_fclick', 'electronico', 'other', 'after', 'habiendo', 'ante', 'teníais', 'into', 'an', 'seréis', 'bzwz', 'se', 'estuvo', 'tuviera', 'fuésemos', 'nuestra', 'column', 'can', 'han', \"couldn't\", 'fuesen', 'fuerais', 'estuvieran', 'blue', 'click', 'tienes', 'against', 'hube', 'están', 'estuvieron', 'was', 'tuvieseis', 'hoy', 'grupos', 'otra', 'there', 'wouldn', 'fueron', 'all', 'should', 'estaba', 'these', 'com mx', 'color', 'are', 'encontrar', 'of', 'd', 'está', 'tus', 'un', 'he', 'correo', 'estuvierais', 'adjunto', 'media', 'saludos cordiales', 'hubimos', 'nor', 'bzwz_fzkv', 'margin', 'estad', 'le', 'nos', 'tuyos', 'si', 'than', 'estar', 'estuviese', 'somos', 'estaré', 'until', 'any', \"that'll\", 'with', 'sintiendo', 'hayamos', 'center', \"hadn't\", 'estando', 'fui', 'buen', 'tuvieras', 'aerobu', 'because', 'ellas', 'evehu', \"don't\", 'quienes', 'unos', 'hubieron', 'tengamos', 'ct_sendgrid', 'very', 'fuisteis', 'off', 'haya', 'hadn', 'estuviera', 'no', 'estaréis', 'sean', 'estuvimos', 'hubierais', 'mightn', 'upn', 'doesn', 'yourselves', 'por', 'habías', 'http', 'vosotras', 'mustn', 'al', 'más', 'estaría', 'has', 'estuviesen', 'height', 'down', \"she's\", 'is', 'podrás encontrar', 'my', 'tuviesen', 'px', 'couldn', 'com', 'youtube', 'las', 'día', 'themselves', 'hubisteis', 'their', 'tuve', 'tuviste', 'urldefense', 'teníamos', 's', 'mailsg', 'siguenos', 'img', 'tuyas', 'vuestro', 'width', 'la', \"shouldn't\", \"aren't\", 'estabais', 'quisiera', 'url', 'that', \"mustn't\", 'cuando', 'this', 'out', 'fue', 'mx', 'tuvimos', 'over', 'estas', 'habíais', 'td', 'him', 'too', 'dia', 'buenos', 'nam_safelinks', \"you're\", 'you', 'estuviésemos', 'vuestros', 'su', 'left', 'hubieran', 'wasn', \"haven't\", 'desde', 'll', 'font', 'had', 'teniendo', 'seas', 'sentidos', 'vuestras', 'habría', 'suyas', 'fueses', \"you'd\", 'style', 'don', 'further', 'tarde', 'screen', 'nuestras', 'tenemos', 'fuiste', 'sería', 'estaríais', 'fzkv_jo', 'estarás', \"won't\", 'ya', 'serías', 'fueseis', 'estuvieses', 'habéis', 'itinerary_assets', 'estarían', 'i', 'then', 'eso', 'text', 'quien', 'blueaerolinea', 'or', 'habrán', 'esto', 'algunos', 'poco', 'when', 'mi', 'hers', 'between', 'ma', 'tú', 'image', 'she', 'tendría', 'under', 'saludos', 'align', 'to', 'nosotros', 'above', 'tengo', 'myself', 'eras', 'estadas', 'it', 'estará', 'otro', 'seríamos', 'some', 'aren', 'tuyo', 'como', 'entre', 'lydnuvk', 'éramos', 'antes', 'ha', 'mailto', \"should've\", 'tendrías', 'am', 'protection_outlook', 'tuviéramos', 'estos', 'hemos', 'que', 'furl', 'buenas', 'estemos', 'habidas', 'tu', 'yourself', 'nada', 'so', 'hayas', 'being', 'cordiales', 'tendrán', 'hubiesen', 'estés', 'estoy', 'serían', 'habían', 'nosotras', 'have', 'ustedes', 'tuvo', 'who', 'condiciones', 'tendremos', 'fuéramos', 'sentido', '6c756af0', 'habida', 'pdf', 'suyo', 'will', 'con', 'his', \"it's\", 't', 'below', 'estáis', 'ellos', 'did', 'and', 'once', 'hubieseis', 'estuvisteis', 'hubiese', 'seamos', 'here', 'míos', 'doing', 'habrá', 'family', 'para', 'sobre', 'they', 'each', 'tuvieron', 'tuvierais', 'vosotros', 'fueran', 'fueras', 'tendríamos', 'weren', 'habrás', 'tenían', 'hubiste', 'también', 've', 'hola', 'mail', 'pero', 'bz', 'habíamos', 'tenida', 'puedo', 'tengas', 'her', 'tuviese', 'xx ', 'erais', 'hay', 'on', 'mí', 'ti', 'el', 'hayan', 'habríais', 'mucho', 'tendrás', 'es', 'mas', 'estaríamos', 'durante', 'didn', 'why', 'hubieras', 'not', 'tendré', 'espera', 'podrás', 'seáis', 'gracias', 'while', 'both', 'saber', 'such', 'muchos', 'tengan', 'for', 'were', 'whom', 'había', 'himself', '01d57aa3', 'tenido', 'términos condiciones', 'estuvieseis', 'serás', 'tardes', 'este', 'ese', \"wouldn't\", 'del', 'fuera', 'sentida', 'your', 'eres', 'gmail com', 'about', 'él', 'few', 'fupn_dhn', 'we'}\n"
     ]
    }
   ],
   "source": [
    "# Define initial stop words sets\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_sp = set(stopwords.words('spanish'))\n",
    "\n",
    "# Add additional Spanish stopwords\n",
    "new_stopwords = []\n",
    "with open('list_stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    new_stopwords = json.load(file)\n",
    "\n",
    "# Combine all stopwords\n",
    "all_stopwords = stop_words_en.union(stop_words_sp).union(set(new_stopwords))\n",
    "\n",
    "# Define stopwords excluding certain words if needed\n",
    "not_stopwords = set() # This should be a set of words you want to exclude from the stop words\n",
    "final_stop_words = set([word for word in all_stopwords if word not in not_stopwords])\n",
    "\n",
    "print(final_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    # return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in final_stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('comunicate', 1),\n",
       "  ('emergencias', 1),\n",
       "  ('escribirnos_recibido', 1),\n",
       "  ('respuesta', 1),\n",
       "  ('seguida_daremos', 1)]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LDA model\n",
    "lda_model.save(\"lda_model\")\n",
    "\n",
    "# Optionally, you can also save the corpus and id2word dictionary for later use\n",
    "# Save corpus\n",
    "gensim.corpora.MmCorpus.serialize('corpus.mm', corpus)\n",
    "\n",
    "# Save id2word dictionary\n",
    "id2word.save('id2word.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel.load(\"lda_model\")\n",
    "\n",
    "# Load corpus\n",
    "corpus = gensim.corpora.MmCorpus('corpus.mm')\n",
    "\n",
    "# Load id2word dictionary\n",
    "id2word = gensim.corpora.Dictionary.load('id2word.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.040*\"monterrey\" + 0.037*\"mexico\" + 0.037*\"julio\" + 0.034*\"ciudad\" + '\n",
      "  '0.028*\"cancun\" + 0.020*\"enviado\" + 0.016*\"viaje\" + 0.016*\"pendiente\" + '\n",
      "  '0.014*\"agosto\" + 0.012*\"guadalajara\"'),\n",
      " (1,\n",
      "  '0.045*\"vuelo\" + 0.016*\"mxn\" + 0.015*\"reservacion\" + 0.010*\"horas\" + '\n",
      "  '0.010*\"salida\" + 0.009*\"equipaje\" + 0.009*\"aeropuerto\" + 0.008*\"pasajeros\" '\n",
      "  '+ 0.008*\"clave\" + 0.007*\"vuelos\"'),\n",
      " (2,\n",
      "  '0.041*\"cotizacion\" + 0.019*\"viaje\" + 0.011*\"tarifa\" + 0.011*\"pago\" + '\n",
      "  '0.010*\"informacion\" + 0.009*\"email\" + 0.009*\"aerobus\" + 0.009*\"fecha\" + '\n",
      "  '0.008*\"servicio\" + 0.008*\"equipo\"'),\n",
      " (3,\n",
      "  '0.017*\"datos\" + 0.011*\"maleta\" + 0.010*\"solicito\" + 0.008*\"espero\" + '\n",
      "  '0.008*\"hora\" + 0.007*\"pueden\" + 0.007*\"momento\" + 0.006*\"cdmx\" + '\n",
      "  '0.005*\"hrs\" + 0.005*\"vuelo\"'),\n",
      " (4,\n",
      "  '0.061*\"flight\" + 0.018*\"please\" + 0.017*\"airport\" + 0.015*\"get\" + '\n",
      "  '0.014*\"usd\" + 0.012*\"reservation\" + 0.011*\"need\" + 0.011*\"baggage\" + '\n",
      "  '0.010*\"travel\" + 0.010*\"bag\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 4 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df['enhanced_cleaned_descripcion']` is your cleaned text column\n",
    "texts1 = df['cleaned_descripcion'].apply(lambda x: x.split())\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(texts1)\n",
    "\n",
    "# Filter out words that occur less frequently\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Convert document into the bag-of-words (BoW) format = list of (token_id, token_count)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el86401397064782300968115538683\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el86401397064782300968115538683_data = {\"mdsDat\": {\"x\": [0.1015046981855555, 0.20352449184173177, 0.19759049762208375, -0.33145885741902703, -0.17116083023034406], \"y\": [0.06185801634380686, -0.05898671860528317, -0.09450789386474147, -0.2500643395531557, 0.3417009356793735], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [NaN, NaN, NaN, NaN, NaN]}, \"tinfo\": {\"Term\": [\"comunicate\", \"emergencias\", \"escribirnos_recibido\", \"respuesta\", \"seguida_daremos\", \"ademas\", \"adicional\", \"aleyda_garza\", \"anexamos\", \"aplicada\", \"araujo\", \"asi\", \"aspi\", \"brindo\", \"call\", \"caso\", \"certificado\", \"clientes\", \"confusion\", \"correspondiente\", \"costo\", \"cualquier\", \"debe\", \"dias\", \"disculpamos\", \"excedente_residual\", \"fecha\", \"generada\", \"genere\", \"hacerlo\", \"comunicate\", \"emergencias\", \"escribirnos_recibido\", \"respuesta\", \"seguida_daremos\", \"ademas\", \"adicional\", \"aleyda_garza\", \"anexamos\", \"aplicada\", \"araujo\", \"asi\", \"aspi\", \"brindo\", \"call\", \"caso\", \"certificado\", \"clientes\", \"confusion\", \"correspondiente\", \"costo\", \"cualquier\", \"debe\", \"dias\", \"disculpamos\", \"excedente_residual\", \"fecha\", \"generada\", \"genere\", \"hacerlo\", \"comunicate\", \"emergencias\", \"escribirnos_recibido\", \"respuesta\", \"seguida_daremos\", \"ademas\", \"adicional\", \"aleyda_garza\", \"anexamos\", \"aplicada\", \"araujo\", \"asi\", \"aspi\", \"brindo\", \"call\", \"caso\", \"certificado\", \"clientes\", \"confusion\", \"correspondiente\", \"costo\", \"cualquier\", \"debe\", \"dias\", \"disculpamos\", \"excedente_residual\", \"fecha\", \"generada\", \"genere\", \"hacerlo\", \"comunicate\", \"emergencias\", \"escribirnos_recibido\", \"respuesta\", \"seguida_daremos\", \"ademas\", \"adicional\", \"aleyda_garza\", \"anexamos\", \"aplicada\", \"araujo\", \"asi\", \"aspi\", \"brindo\", \"call\", \"caso\", \"certificado\", \"clientes\", \"confusion\", \"correspondiente\", \"costo\", \"cualquier\", \"debe\", \"dias\", \"disculpamos\", \"excedente_residual\", \"fecha\", \"generada\", \"genere\", \"hacerlo\", \"comunicate\", \"emergencias\", \"escribirnos_recibido\", \"respuesta\", \"seguida_daremos\", \"ademas\", \"adicional\", \"aleyda_garza\", \"anexamos\", \"aplicada\", \"araujo\", \"asi\", \"aspi\", \"brindo\", \"call\", \"caso\", \"certificado\", \"clientes\", \"confusion\", \"correspondiente\", \"costo\", \"cualquier\", \"debe\", \"dias\", \"disculpamos\", \"excedente_residual\", \"fecha\", \"generada\", \"genere\", \"hacerlo\", \"comunicate\", \"emergencias\", \"escribirnos_recibido\", \"respuesta\", \"seguida_daremos\", \"ademas\", \"adicional\", \"aleyda_garza\", \"anexamos\", \"aplicada\", \"araujo\", \"asi\", \"aspi\", \"brindo\", \"call\", \"caso\", \"certificado\", \"clientes\", \"confusion\", \"correspondiente\", \"costo\", \"cualquier\", \"debe\", \"dias\", \"disculpamos\", \"excedente_residual\", \"fecha\", \"generada\", \"genere\", \"hacerlo\"], \"Freq\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"Total\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -14.6563, -14.8319, -14.8372, -14.7944, -14.8372, -14.8151, -14.8176, -14.8045, -14.8, -14.6275, -14.8372, -14.8092, -14.8372, -14.7922, -14.815, -14.8163, -14.8176, -14.8134, -14.8258, -14.8249, -6.1719, -14.8057, -14.821, -6.1437, -14.8372, -14.6992, -14.7946, -14.8091, -14.8174, -14.8084, -16.498, -16.6039, -16.6142, -5.3367, -16.6142, -16.587, -6.5646, -16.5189, -16.5949, -15.2743, -16.4714, -6.4038, -16.6142, -16.5873, -6.9239, -5.4517, -6.7846, -6.673, -10.315, -6.165, -6.1712, -7.3317, -6.756, -5.1459, -16.6142, -15.9256, -5.0656, -16.5795, -10.3359, -7.9914, -15.3759, -7.1457, -15.2529, -6.988, -15.2529, -15.4279, -15.4064, -15.3995, -15.4204, -15.4448, -15.4489, -5.8763, -15.4489, -15.4181, -8.6187, -5.5826, -15.3646, -15.3787, -15.4337, -15.4334, -15.3782, -4.9971, -8.8728, -4.894, -15.4489, -14.5456, -4.7644, -8.0807, -15.4185, -15.3531, -10.3437, -15.366, -15.38, -15.3238, -15.38, -6.0387, -15.354, -15.3101, -9.2899, -12.9874, -15.3803, -15.3085, -15.3806, -8.0004, -15.3476, -15.3554, -15.3491, -15.3249, -15.3038, -15.3514, -15.347, -15.3242, -15.3489, -15.3515, -15.3806, -14.8062, -15.3467, -15.3205, -15.3313, -15.2908, -13.5216, -13.5214, -13.5216, -13.5185, -13.5216, -13.5185, -13.5212, -13.5216, -13.5216, -13.5216, -13.5216, -13.5194, -13.5216, -13.5216, -9.2872, -9.6261, -13.5215, -13.5058, -13.4845, -13.5208, -13.5211, -13.5178, -13.5109, -13.5182, -13.5216, -13.5216, -13.5187, -13.5216, -13.5187, -13.5215], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN]}, \"token.table\": {\"Topic\": [], \"Freq\": [], \"Term\": []}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el86401397064782300968115538683\", ldavis_el86401397064782300968115538683_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el86401397064782300968115538683\", ldavis_el86401397064782300968115538683_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el86401397064782300968115538683\", ldavis_el86401397064782300968115538683_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster  Freq\n",
       "topic                                           \n",
       "0      0.101505  0.061858       1        1   NaN\n",
       "1      0.203524 -0.058987       2        1   NaN\n",
       "2      0.197590 -0.094508       3        1   NaN\n",
       "3     -0.331459 -0.250064       4        1   NaN\n",
       "4     -0.171161  0.341701       5        1   NaN, topic_info=                    Term  Freq  Total Category  logprob  loglift\n",
       "0             comunicate   0.0    0.0  Default  30.0000     30.0\n",
       "1            emergencias   0.0    0.0  Default  29.0000     29.0\n",
       "2   escribirnos_recibido   0.0    0.0  Default  28.0000     28.0\n",
       "3              respuesta   0.0    0.0  Default  27.0000     27.0\n",
       "4        seguida_daremos   0.0    0.0  Default  26.0000     26.0\n",
       "..                   ...   ...    ...      ...      ...      ...\n",
       "25    excedente_residual   0.0    0.0   Topic5 -13.5216      NaN\n",
       "26                 fecha   0.0    0.0   Topic5 -13.5187      NaN\n",
       "27              generada   0.0    0.0   Topic5 -13.5216      NaN\n",
       "28                genere   0.0    0.0   Topic5 -13.5187      NaN\n",
       "29               hacerlo   0.0    0.0   Topic5 -13.5215      NaN\n",
       "\n",
       "[180 rows x 6 columns], token_table=Empty DataFrame\n",
       "Columns: [Topic, Freq, Term]\n",
       "Index: [], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "\n",
    "# Fit and transform the cleaned descriptions to a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(df['cleaned_descripcion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of clusters\n",
    "num_clusters = 5  # This is an arbitrary choice and should be optimized\n",
    "\n",
    "# Initialize K-Means and fit it to the TF-IDF matrix\n",
    "km = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "# Assign the cluster labels to the DataFrame\n",
    "clusters = km.labels_.tolist()\n",
    "df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: bebé preguntar quería debe equipaje ser equivoco er eqvc equívoco\n",
      "Cluster 1: noches buenas hola ola tal vuelo pregunta disculpe puedo quisiera\n",
      "Cluster 2: image monterrey comprar méxico compra twitter ciudad forma blue cancún\n",
      "Cluster 3: hola important blueaerolinea buenas tardes vuelo día buen gracias blue\n",
      "Cluster 4: vuelo mxn horas salida si minutos tua tarifa aeropuerto podrás\n"
     ]
    }
   ],
   "source": [
    "# Print top terms per cluster\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:  # replace 10 with n words per cluster\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
