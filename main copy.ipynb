{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Customer Feedback Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "The airline industry is highly competitive and customer service is a critical area that can influence customer loyalty and brand reputation. Timely and effective handling of customer feedback is crucial for maintaining service quality. Analyzing customer feedback can provide insights into common concerns and areas for improvement.\n",
    "\n",
    "### Project Goals\n",
    "\n",
    "The primary goal of this project is twofold:\n",
    "\n",
    "1. **Sentiment Analysis**: To automatically determine the sentiment of customer comments. The sentiment can be:\n",
    "   - **Negative (Complaint)**: Indicates the customer faced an issue or is dissatisfied.\n",
    "   - **Positive (Congratulation)**: Reflects customer satisfaction or a positive experience.\n",
    "   - **Neutral (Opinion)**: Represents a customer's opinion that is neither a direct complaint nor praise.\n",
    "\n",
    "2. **Complaint Classification**: To classify customer feedback into categories for routing to the appropriate department. This classification will help in streamlining the process of addressing customer feedback efficiently. Example categories include:\n",
    "   - **Luggage Issues**: For complaints or feedback related to damaged or lost luggage.\n",
    "   - **Flight Delays**: For issues pertaining to delayed or cancelled flights.\n",
    "   - **Billing Issues**: For concerns related to overcharges, refunds, or payment discrepancies.\n",
    "\n",
    "The end product will be a model or a set of models that can accurately tag incoming customer comments with the correct sentiment and category, allowing the customer service team to promptly address and resolve the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os.path\n",
    "import json\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import little_mallet_wrapper as lmw\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "path_to_mallet = 'mallet-2.0.8/bin/mallet'  # CHANGE THIS TO YOUR MALLET PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gabot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_excel('output.xlsx', engine='openpyxl')\n",
    "\n",
    "# Change typo in column name\n",
    "data.rename(columns={'descripciom': 'descripcion'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gracias por escribirnos, hemos recibido tu correo y en seguida daremos '\n",
      " 'respuesta. Para emergencias, por favor comunicate al 4777920450 ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = data.descripcion.values.tolist()\n",
    "\n",
    "# Remove non-string elements\n",
    "data = [str(sent) for sent in data]\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gracias', 'por', 'escribirnos', 'hemos', 'recibido', 'tu', 'correo', 'en', 'seguida', 'daremos', 'respuesta', 'para', 'emergencias', 'por', 'favor', 'comunicate', 'al']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gracias', 'por', 'escribirnos_hemos_recibido', 'tu', 'correo', 'en', 'seguida_daremos_respuesta', 'para', 'emergencias', 'por', 'favor', 'comunicate', 'al']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'him', 'unos', 'be', 'estás', 'seamos', 'todo', 'estada', 'esa', 'hacer', 'shan', 'once', 'hubieras', 'yourself', 'a', 'hayan', 'algo', 'aren', 'os', 'tenías', 'itinerary_assets', 'ti', 'ct_sendgrid', 'ma', 'seréis', 'tiene', 'tuviste', 'have', 'as', 'was', 'haven', 'mío', 'bz', 'again', 'any', 'poco', 'saber', 'e', 'tuyo', 'hoy', 'teníamos', 'at', 'habrán', 'estuvisteis', 'isn', 'uno', 'fuiste', 'text', 'estuviésemos', 'are', 'width', 'hasta', 'mas', 'estuvo', 'esté', 'px', 'yo', \"it's\", 'noche', 'what', 'favor', '01d57aa3', 'evehu', 'fui', \"shouldn't\", 'by', 'those', 'to', 'algunos', 'días', 'so', 'more', 'tenidos', 'hemos', 'tuvimos', 'eso', 'he', 'tus', 'estuvieses', 'el', 'them', 'who', 'estaríais', 'qué', 'estad', 'más', 'fuera', 'tengamos', \"should've\", 'ante', \"wouldn't\", 'otro', 'estuvimos', 'in', 'vosotros', 'own', 'estén', 'such', 'size', 'sean', 'podrás', 'mi', 'estaba', 'youtube', 'eras', 'habías', 'seáis', 'electronico', \"hasn't\", 'que', 'm', 'tuviera', 'había', 'fls_fclick', \"she's\", 'tenidas', 'sentida', 'align', 'i', 'www', 'feature', 'height', 'its', 'fueron', 'estuvieseis', 'an', 'hubierais', 'these', 'vuestro', 'habrás', 'vosotras', 'ustedes', 'few', 'muchas', 'vuestra', 'family', \"aren't\", 'tengas', 'did', 'hayas', 'his', 'teníais', 'himself', 'sin', 'during', 'habidas', 'tenían', 'entre', 'ni', 'nosotras', 'sentidas', 'estado', 'your', 'hn_evehu', 'bzwz_fzkv', 'fue', 'ella', 'the', 'hubiese', 'él', 'is', 'on', 'estaban', 'img', 'why', 'habían', 'hubiésemos', 'tuyos', 'seremos', 'su', 'seríamos', 'vuestras', \"you'd\", 'just', 'seré', 'puedo', 'estuvieran', 'tarde', 'esta', '6c756af0', 'tanto', 'como', 'tuve', 'gracias', 'itself', 'style', 'their', 'until', 'quisiera', 'serán', 'ourselves', 'has', 'estados', 'tú', 'bzwz', 'ustede', 'términos condiciones', 'auto', 'hayamos', 'when', 'herself', 'tuyas', 'serías', 'fuéramos', 'o', 'habríamos', 'erais', 'it', 'le', 'tuya', 'before', 'habrían', 'which', 'correo electrónico', 'estaremos', 'tuviéramos', 'tendríamos', 'correo', 'será', 'habríais', 'didn', 'hube', 'fuese', 'most', 'all', 'gmail com', 'cid', 'mí', 'es', 'but', 'están', 'under', 'off', 'han', 'habéis', 'estuvieras', 'está', 'habrá', 'content', 'tendré', 'fuésemos', 'fzkv_jo', 'needn', 'muchos', 'do', 'en', \"that'll\", 'habremos', 'weren', \"won't\", 'estaría', 'tuviesen', 'estemos', 'serían', 'saludos', 'this', 'fuimos', 'there', 'fueras', 'tienen', 'urldefense', 'doesn', 'won', 'nuestro', 'had', 'desde', 'si', 'estábamos', 'ha', \"couldn't\", 'only', \"hadn't\", 'fuesen', \"don't\", 'esto', 'habidos', 'now', 'into', 'habíamos', 'habido', 'buen', 'while', 'table', 'fuisteis', 'éramos', 'left', 'xx ', 'here', 'image004', 'te', 'hubieses', 'protection_outlook', 'hubieran', 'fueran', 'estuviera', 'tenía', 'hotmail', 'her', 'los', 'sería', 'wouldn', 'tuvieras', 'nuestras', 'tuvieron', 'am', 'contra', 'grupos', 'suya', 'shouldn', 'tienes', 'font', 'una', \"shan't\", 'hubisteis', 'ours', 'because', 'eres', 'mail_itinerary', 'habréis', 'doing', 'habrías', 'up', 'sus', 'tengáis', 'tu', 'tendríais', 'don', 'png', 'div', \"haven't\", 'lo', 'she', 'they', 'image', 'hubieseis', 'https', 'noches', 'net', 'aerobu', 'habida', 's', 'estuviste', \"didn't\", 'with', 'blueaerolinea', 'formato', 'some', 'tuvierais', 'tuvieran', 'if', 'both', 'algunas', 'mx', 'not', 'estabas', 'adjunto podrá', 'tardes', 'myself', 'estabais', 't', 'further', 'com', 'mustn', 'theirs', 're', 'ya', 'del', 'mailsg', 'then', 'dia', 'otras', 'estamos', 'sentid', 'haya', 'over', 'seas', 'yourselves', 'about', 'we', 'ls', 'estar', 'habiendo', 'does', 'no', 'furl', 'same', 'estarían', 'below', 'son', 'tenemos', 'themselves', 'día', 'suyas', \"you've\", 'era', 'míos', 'you', 'nuestros', 'tendrás', 'hubiesen', 'other', 'antes', 'tenga', 'otra', 'how', 'fupn_dhn', 'estarías', 'otros', 'upn', 'ellas', 'tuvo', 'tengo', 'against', 'wasn', 'estas', 'por', 'estando', 'condiciones', 'se', \"weren't\", 'este', 'estoy', 'soy', 'estuve', 'our', 'las', 'nos', 'nbsp', \"wasn't\", 'each', 'estaríamos', 'ellos', 'serás', 'tuvieses', 'sentidos', \"mustn't\", 'll', 'http', 'tr', 'estadas', 'tendría', 'td', 'were', 've', 'somos', 'above', \"needn't\", 'tendrán', 'estuviese', 'sobre', 'center', 'quien', 'estáis', 'tuviésemos', 'gmail', 'suyos', 'cognitoforms', 'too', 'tendrías', 'nosotros', 'hay', 'porque', 'teniendo', 'estés', 'hubiera', 'of', 'buenas', 'where', 'down', 'pero', 'for', 'la', 'whom', 'tenido', 'seríais', 'donde', 'de', 'after', 'estéis', \"doesn't\", 'mucho', 'habría', 'estuviéramos', 'hubieron', 'buenos', 'tenéis', 'hubimos', 'hers', 'tendremos', 'podrá', 'nam_safelinks', 'being', 'nor', 'ese', 'muy', 'me', 'quienes', 'cual', 'estuviesen', \"you're\", 'suyo', 'lydnuvk', 'hubo', 'cordiales', 'con', 'tenida', 'mightn', \"mightn't\", 'estarán', 'hubiéramos', \"you'll\", 'durante', 'url', 'through', 'than', 'can', 'yours', 'esas', 'sois', 'important', 'términos', 'fueses', 'todos', 'tendrían', 'nada', 'estuvierais', 'mías', 'y', 'or', 'estará', 'tened', 'vea', 'my', 'cuando', 'column', 'estaréis', 'siente', 'hola', 'very', 'tengan', 'd', 'estos', 'mía', 'nuestra', 'tendréis', 'saludos cordiales', 'solo', 'com mx', 'les', 'hubiste', 'fueseis', 'sea', 'también', 'been', 'blue', 'sí', 'podrás encontrar', 'and', 'out', 'eran', 'hayáis', 'fuerais', 'estaré', 'click', 'para', 'tendrá', 'between', 'sentido', 'will', 'encontrar', 'hasn', 'mis', 'vuestros', 'having', 'estarás', 'couldn', 'al', 'tuvisteis', 'hadn', 'quedo', 'mailto', 'should', 'tuvieseis', 'margin', 'habíais', 'that', 'tuviese', \"isn't\", 'espera', 'habré', 'mail', 'sintiendo', 'un', 'pdf', 'ain', 'adjunto', 'estuvieron', 'from', 'esos'}\n"
     ]
    }
   ],
   "source": [
    "# Define initial stop words sets\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_sp = set(stopwords.words('spanish'))\n",
    "\n",
    "# Add additional Spanish stopwords\n",
    "new_stopwords = []\n",
    "with open('list_stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    new_stopwords = json.load(file)\n",
    "\n",
    "# Combine all stopwords\n",
    "all_stopwords = stop_words_en.union(stop_words_sp).union(set(new_stopwords))\n",
    "\n",
    "# Define stopwords excluding certain words if needed\n",
    "not_stopwords = set() # This should be a set of words you want to exclude from the stop words\n",
    "final_stop_words = set([word for word in all_stopwords if word not in not_stopwords])\n",
    "\n",
    "print(final_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    # return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in final_stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('comunicate', 1),\n",
       "  ('emergencias', 1),\n",
       "  ('escribirnos', 1),\n",
       "  ('recibido', 1),\n",
       "  ('respuesta', 1),\n",
       "  ('seguida_daremos', 1)]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LDA model\n",
    "lda_model.save(\"lda_model\")\n",
    "\n",
    "# Optionally, you can also save the corpus and id2word dictionary for later use\n",
    "# Save corpus\n",
    "gensim.corpora.MmCorpus.serialize('corpus.mm', corpus)\n",
    "\n",
    "# Save id2word dictionary\n",
    "id2word.save('id2word.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel.load(\"lda_model\")\n",
    "\n",
    "# Load corpus\n",
    "corpus = gensim.corpora.MmCorpus('corpus.mm')\n",
    "\n",
    "# Load id2word dictionary\n",
    "id2word = gensim.corpora.Dictionary.load('id2word.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.011*\"protection_outlook\" + 0.010*\"reserved\" + 0.006*\"data\" + '\n",
      "  '0.006*\"fls_fclick\" + 0.006*\"entrada\" + 0.006*\"detalles\" + 0.005*\"furl\" + '\n",
      "  '0.005*\"nam_safelinks\" + 0.005*\"evehu\" + 0.005*\"fupn_dhn\"'),\n",
      " (1,\n",
      "  '0.025*\"vuelo\" + 0.016*\"mxn\" + 0.014*\"itinerary_assets\" + 0.010*\"hn_evehu\" + '\n",
      "  '0.008*\"fzkv_jo\" + 0.007*\"salida\" + 0.007*\"reservacion\" + 0.006*\"horas\" + '\n",
      "  '0.005*\"uso\" + 0.005*\"pixel_gif\"'),\n",
      " (2,\n",
      "  '0.007*\"yiv_yiv\" + 0.003*\"yiv\" + 0.003*\"color\" + 0.003*\"line\" + '\n",
      "  '0.002*\"begin\" + 0.002*\"right\" + 0.002*\"serif\" + 0.002*\"sans\" + '\n",
      "  '0.002*\"border\" + 0.002*\"padding\"'),\n",
      " (3,\n",
      "  '0.016*\"vuelo\" + 0.008*\"cotizacion\" + 0.006*\"julio\" + 0.006*\"reservacion\" + '\n",
      "  '0.006*\"viaje\" + 0.005*\"fecha\" + 0.005*\"nombre\" + 0.004*\"mexico\" + '\n",
      "  '0.004*\"pago\" + 0.004*\"clave\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 4 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102966"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
